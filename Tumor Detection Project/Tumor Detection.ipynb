{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2bddb8c",
   "metadata": {},
   "source": [
    "# Tumor Detection Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42454e2d",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "It takes the vector and fits it into the sigmoid function, and result in the value ranging between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, w, b):\n",
    "    z = np.dot(x[i], w) + b\n",
    "    g = 1/(1+np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929b06b",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "This function helps us calculate the cost of the sigmoid function we used inorder to analyse, how good out model is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2a0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cost(x, y, w, b, lambda_ = 1):\n",
    "    m, n = x.shape\n",
    "    cost = 0.0 \n",
    "    \n",
    "    for i in range(m):\n",
    "        g = sigmoid(x[i], w, b)\n",
    "        cost += (-y[i]*np.log(g) - (1-y[i])*np.log(1-g))\n",
    "    cost = cost/m\n",
    "        \n",
    "    # regularizing the model to avoid overfitting\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += w[j]**2\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost\n",
    "        \n",
    "    total_cost = cost + reg_cost\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1aa87",
   "metadata": {},
   "source": [
    "### Gradient for this logstic regression\n",
    "This function helps to calculate the derivative of the cost function with respect to weights and bias. This helps us understand that where do we need to move inorder to minimize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38dbae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_logistic(x, y, w, b):\n",
    "    m, n = x.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        g = sigmoid(x[i], w, b)\n",
    "        err = g - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err*x[i][j]\n",
    "        dj_db[j] += err\n",
    "    \n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eeba9d",
   "metadata": {},
   "source": [
    "### Gradient Descent for Logistic Regression\n",
    "This algorithm let us choose a proper set of weights and bias that would be associated with the feature inputs. It helps us to get certain values of parameters that will help us to reduce the cost function, the overall cost of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489baf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, iterations):\n",
    "    \n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        dj_dw, dj_db = gradient(x, y, w, b)\n",
    "        \n",
    "        w = w - alpha*dj_dw\n",
    "        b = b - alpha*dj_db\n",
    "        \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41baba0d",
   "metadata": {},
   "source": [
    "### Main Function\n",
    "All inputs and variable initialization is done in this region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e99c81a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the default inputs: [0.01862297 0.02057229 0.02272091 0.98463772 0.99849336 0.97711696]\n",
      "Prediction for [1, 2]: 0.7712058186327441\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x, w, b):\n",
    "    z = np.dot(x, w) + b\n",
    "    g = 1/(1+np.exp(-z))\n",
    "    return g\n",
    "\n",
    "def logistic_cost(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    cost = 0.0 \n",
    "    \n",
    "    for i in range(m):\n",
    "        g = sigmoid(x[i], w, b)\n",
    "        cost += (-y[i]*np.log(g) - (1-y[i])*np.log(1-g))\n",
    "        \n",
    "    cost = cost/m\n",
    "    return cost\n",
    "\n",
    "def compute_gradient_logistic(X, y, w, b): \n",
    "\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros_like(X[0])                           #(n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(X[i], w, b) \n",
    "        err_i  = f_wb_i  - y[i]             \n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "        \n",
    "    return dj_db, dj_dw  \n",
    "\n",
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
    "  \n",
    "    w = w_in \n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "        \n",
    "    return w, b  \n",
    "\n",
    "x_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1]) \n",
    "\n",
    "w_init = np.zeros_like(x_train[0])\n",
    "b_init = 0.\n",
    "alpha = 0.1\n",
    "iterations = 10000\n",
    "\n",
    "w_new, b_new = gradient_descent(x_train, y_train, w_init, b_init, alpha, iterations)\n",
    "\n",
    "# Generating the predictions for the given input by our model\n",
    "print(f\"Prediction for the default inputs: {sigmoid(x_train, w_new, b_new)}\")\n",
    "print(f\"Prediction for [1, 2]: {sigmoid(np.array([1, 2]), w_new, b_new)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59121fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
