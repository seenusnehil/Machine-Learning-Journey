{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2bddb8c",
   "metadata": {},
   "source": [
    "# Tumor Detection Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42454e2d",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "It takes the value of the function we got from the model function above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, w, b):\n",
    "    m = x.shape[0]\n",
    "    for i in range(m):\n",
    "        z = np.dot(x[i], w) + b\n",
    "    \n",
    "    g = 1/(1+np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929b06b",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "This function helps us calculate the cost of the sigmoid function we used inorder to analyse, how good out model is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2a0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cost(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    cost = 0.0 \n",
    "    \n",
    "    for i in range(m):\n",
    "        g = sigmoid(x[i], w, b)\n",
    "        cost += (-y[i]*np.log(g) - (1-y[i])*np.log(1-g))\n",
    "        \n",
    "    cost = cost/m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1aa87",
   "metadata": {},
   "source": [
    "### Gradient for this logstic regression\n",
    "This function helps to calculate the derivative of the cost function with respect to weights and bias. This helps us understand that where do we need to move inorder to minimize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38dbae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_logistic(x, y, w, b):\n",
    "    m, n = x.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        g = sigmoid(x[i], w, b)\n",
    "        err = g - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err*x[i][j]\n",
    "        dj_db[j] += err\n",
    "    \n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eeba9d",
   "metadata": {},
   "source": [
    "### Gradient Descent for Logistic Regression\n",
    "This algorithm let us choose a proper set of weights and bias that would be associated with the feature inputs. It helps us to get certain values of parameters that will help us to reduce the cost function, the overall cost of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489baf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, iterations):\n",
    "    \n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        dj_dw, dj_db = gradient(x, y, w, b)\n",
    "        \n",
    "        w = w - alpha*dj_dw\n",
    "        b = b - alpha*dj_db\n",
    "        \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41baba0d",
   "metadata": {},
   "source": [
    "### Main Function\n",
    "All inputs and variable initialization is done in this region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e99c81a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 63>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     61\u001b[0m iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m---> 63\u001b[0m w_new, b_new \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(w_new, b_new)\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(X, y, w_in, b_in, alpha, num_iters)\u001b[0m\n\u001b[0;32m     43\u001b[0m b \u001b[38;5;241m=\u001b[39m b_in\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iters):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Calculate the gradient and update the parameters\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     dj_db, dj_dw \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gradient_logistic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Update Parameters using w, b, alpha and gradient\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m-\u001b[39m alpha \u001b[38;5;241m*\u001b[39m dj_dw               \n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mcompute_gradient_logistic\u001b[1;34m(X, y, w, b)\u001b[0m\n\u001b[0;32m     31\u001b[0m     err_i  \u001b[38;5;241m=\u001b[39m f_wb_i  \u001b[38;5;241m-\u001b[39m y[i]             \n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m---> 33\u001b[0m         \u001b[43mdj_dw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m dj_dw[j] \u001b[38;5;241m+\u001b[39m err_i \u001b[38;5;241m*\u001b[39m X[i,j]      \u001b[38;5;66;03m#scalar\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     dj_db \u001b[38;5;241m=\u001b[39m dj_db \u001b[38;5;241m+\u001b[39m err_i\n\u001b[0;32m     35\u001b[0m dj_dw \u001b[38;5;241m=\u001b[39m dj_dw\u001b[38;5;241m/\u001b[39mm                                   \u001b[38;5;66;03m#(n,)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x, w, b):\n",
    "    m = x.shape[0]\n",
    "    for i in range(m):\n",
    "        z = np.dot(x[i], w) + b\n",
    "    \n",
    "    g = 1/(1+np.exp(-z))\n",
    "    return g\n",
    "\n",
    "def logistic_cost(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    cost = 0.0 \n",
    "    \n",
    "    for i in range(m):\n",
    "        g = sigmoid(x[i], w, b)\n",
    "        cost += (-y[i]*np.log(g) - (1-y[i])*np.log(1-g))\n",
    "        \n",
    "    cost = cost/m\n",
    "    return cost\n",
    "\n",
    "def compute_gradient_logistic(X, y, w, b): \n",
    "\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(X[i], w, b) \n",
    "        err_i  = f_wb_i  - y[i]             \n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "        \n",
    "    return dj_db, dj_dw  \n",
    "\n",
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
    "  \n",
    "    w = w_in \n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "        \n",
    "    return w, b  \n",
    "\n",
    "x_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1]) \n",
    "\n",
    "w_init = np.zeros_like(x_train[0])\n",
    "b_init = 0.\n",
    "alpha = 0.1\n",
    "iterations = 10000\n",
    "\n",
    "w_new, b_new = gradient_descent(x_train, y_train, w_init, b_init, alpha, iterations)\n",
    "\n",
    "print(w_new, b_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59121fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
